From a18f08fee70ef24578dce4241fe5350c641e2d86 Mon Sep 17 00:00:00 2001
From: Nathan Chancellor <nathan@kernel.org>
Date: Wed, 25 May 2022 15:13:06 -0700
Subject: [PATCH 12/17] NOT-CBL: Revert "workqueue: Wrap flush_workqueue()
 using an inline function"

This reverts commit e449c388913ccd36641f7cc0c335029a7cc161f4.

The warnings should be cleaned up before this is added...

Signed-off-by: Nathan Chancellor <nathan@kernel.org>
---
 include/linux/workqueue.h | 61 ++++-----------------------------------
 kernel/workqueue.c        | 16 +++-------
 2 files changed, 9 insertions(+), 68 deletions(-)

diff --git a/include/linux/workqueue.h b/include/linux/workqueue.h
index 7cb62bcedb29..7fee9b6cfede 100644
--- a/include/linux/workqueue.h
+++ b/include/linux/workqueue.h
@@ -445,7 +445,7 @@ extern bool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,
 			struct delayed_work *dwork, unsigned long delay);
 extern bool queue_rcu_work(struct workqueue_struct *wq, struct rcu_work *rwork);
 
-extern void __flush_workqueue(struct workqueue_struct *wq);
+extern void flush_workqueue(struct workqueue_struct *wq);
 extern void drain_workqueue(struct workqueue_struct *wq);
 
 extern int schedule_on_each_cpu(work_func_t func);
@@ -563,23 +563,15 @@ static inline bool schedule_work(struct work_struct *work)
 	return queue_work(system_wq, work);
 }
 
-/*
- * Detect attempt to flush system-wide workqueues at compile time when possible.
- *
- * See https://lkml.kernel.org/r/49925af7-78a8-a3dd-bce6-cfc02e1a9236@I-love.SAKURA.ne.jp
- * for reasons and steps for converting system-wide workqueues into local workqueues.
- */
-extern void __warn_flushing_systemwide_wq(void)
-	__compiletime_warning("Please avoid flushing system-wide workqueues.");
-
 /**
  * flush_scheduled_work - ensure that any scheduled work has run to completion.
  *
  * Forces execution of the kernel-global workqueue and blocks until its
  * completion.
  *
- * It's very easy to get into trouble if you don't take great care.
- * Either of the following situations will lead to deadlock:
+ * Think twice before calling this function!  It's very easy to get into
+ * trouble if you don't take great care.  Either of the following situations
+ * will lead to deadlock:
  *
  *	One of the work items currently on the workqueue needs to acquire
  *	a lock held by your code or its caller.
@@ -594,53 +586,10 @@ extern void __warn_flushing_systemwide_wq(void)
  * need to know that a particular work item isn't queued and isn't running.
  * In such cases you should use cancel_delayed_work_sync() or
  * cancel_work_sync() instead.
- *
- * Please stop calling this function! A conversion to stop flushing system-wide
- * workqueues is in progress. This function will be removed after all in-tree
- * users stopped calling this function.
  */
 static inline void flush_scheduled_work(void)
 {
-#if !defined(CONFIG_WERROR) && defined(CONFIG_PROVE_LOCKING)
-	/*
-	 * Warn only if emitting warning message does not cause build failure
-	 * and the developer wants warning about possibility of deadlock, for
-	 * there are currently in-tree flush_scheduled_work() users.
-	 */
-	__warn_flushing_systemwide_wq();
-#endif
-	__flush_workqueue(system_wq);
-}
-
-/**
- * flush_workqueue - ensure that any scheduled work has run to completion.
- * @wq: workqueue to flush
- *
- * This function sleeps until all work items which were queued on entry
- * have finished execution, but it is not livelocked by new incoming ones.
- */
-static __always_inline void flush_workqueue(struct workqueue_struct *wq)
-{
-	/*
-	 * Always warn, for there is no in-tree flush_workqueue(system_*_wq)
-	 * user.
-	 */
-	if ((__builtin_constant_p(wq == system_wq) &&
-	     wq == system_wq) ||
-	    (__builtin_constant_p(wq == system_highpri_wq) &&
-	     wq == system_highpri_wq) ||
-	    (__builtin_constant_p(wq == system_long_wq) &&
-	     wq == system_long_wq) ||
-	    (__builtin_constant_p(wq == system_unbound_wq) &&
-	     wq == system_unbound_wq) ||
-	    (__builtin_constant_p(wq == system_freezable_wq) &&
-	     wq == system_freezable_wq) ||
-	    (__builtin_constant_p(wq == system_power_efficient_wq) &&
-	     wq == system_power_efficient_wq) ||
-	    (__builtin_constant_p(wq == system_freezable_power_efficient_wq) &&
-	     wq == system_freezable_power_efficient_wq))
-		__warn_flushing_systemwide_wq();
-	__flush_workqueue(wq);
+	flush_workqueue(system_wq);
 }
 
 /**
diff --git a/kernel/workqueue.c b/kernel/workqueue.c
index 1ea50f6be843..4056f2a3f9d5 100644
--- a/kernel/workqueue.c
+++ b/kernel/workqueue.c
@@ -2788,13 +2788,13 @@ static bool flush_workqueue_prep_pwqs(struct workqueue_struct *wq,
 }
 
 /**
- * __flush_workqueue - ensure that any scheduled work has run to completion.
+ * flush_workqueue - ensure that any scheduled work has run to completion.
  * @wq: workqueue to flush
  *
  * This function sleeps until all work items which were queued on entry
  * have finished execution, but it is not livelocked by new incoming ones.
  */
-void __flush_workqueue(struct workqueue_struct *wq)
+void flush_workqueue(struct workqueue_struct *wq)
 {
 	struct wq_flusher this_flusher = {
 		.list = LIST_HEAD_INIT(this_flusher.list),
@@ -2943,7 +2943,7 @@ void __flush_workqueue(struct workqueue_struct *wq)
 out_unlock:
 	mutex_unlock(&wq->mutex);
 }
-EXPORT_SYMBOL(__flush_workqueue);
+EXPORT_SYMBOL(flush_workqueue);
 
 /**
  * drain_workqueue - drain a workqueue
@@ -2971,7 +2971,7 @@ void drain_workqueue(struct workqueue_struct *wq)
 		wq->flags |= __WQ_DRAINING;
 	mutex_unlock(&wq->mutex);
 reflush:
-	__flush_workqueue(wq);
+	flush_workqueue(wq);
 
 	mutex_lock(&wq->mutex);
 
@@ -6111,11 +6111,3 @@ void __init workqueue_init(void)
 	wq_online = true;
 	wq_watchdog_init();
 }
-
-/*
- * Despite the naming, this is a no-op function which is here only for avoiding
- * link error. Since compile-time warning may fail to catch, we will need to
- * emit run-time warning from __flush_workqueue().
- */
-void __warn_flushing_systemwide_wq(void) { }
-EXPORT_SYMBOL(__warn_flushing_systemwide_wq);
-- 
2.36.1

